{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4af1323-0444-48c1-bc88-b2cc65785975",
   "metadata": {},
   "source": [
    "# SpanMarker cross-validation baseline evaluation\n",
    "\n",
    "Loads the baseline model and applies it to all folds of a cross-validation that is identical to the one specified in `training.py`, for the purpose of comparing performance. Note that KFold is used without randomization (as in the training notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from customized_spanmarker_training import NoTrainPreprocTrainer\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset\n",
    "from sklearn.model_selection import KFold\n",
    "from span_marker import SpanMarkerModel\n",
    "from torch.optim import AdamW\n",
    "from transformers import TrainingArguments, get_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176d39b6-b058-4545-9444-95d68c5398e4",
   "metadata": {},
   "source": [
    "Loading datasets remotely and from parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68be7db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "germeval = load_dataset(\"gwlms/germeval2014\")[\"train\"]\n",
    "germeval = germeval.select_columns([\"tokens\", \"ner_tags\"])\n",
    "krp_19jhd = Dataset.from_parquet(\"krp_19jhd.parquet\")\n",
    "krp_20jhd = Dataset.from_parquet(\"krp_20jhd.parquet\")\n",
    "rrb_19jhd = Dataset.from_parquet(\"rrb_19jhd.parquet\")\n",
    "rrb_20jhd = Dataset.from_parquet(\"rrb_20jhd.parquet\")\n",
    "gszh = Dataset.from_parquet(\"gszh.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dcf223-8430-4917-a7d9-7e8006af9927",
   "metadata": {},
   "source": [
    "Mapping features to the original germeval2014 indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf826ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "krp_19jhd = krp_19jhd.map(features=germeval.features)\n",
    "krp_20jhd = krp_20jhd.map(features=germeval.features)\n",
    "rrb_19jhd = rrb_19jhd.map(features=germeval.features)\n",
    "rrb_20jhd = rrb_20jhd.map(features=germeval.features)\n",
    "gszh = gszh.map(features=germeval.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b139f6-0367-4b58-83ef-083e6ba8997a",
   "metadata": {},
   "source": [
    "Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3deebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "gradient_accumulation_steps = 2\n",
    "train_batch_size = 4\n",
    "n_epochs = 10\n",
    "\n",
    "metrics_overall_dict = collections.defaultdict(list)\n",
    "metrics_per_ent_dict = collections.defaultdict(list)\n",
    "\n",
    "for i, (\n",
    "    (_, krp_19jhd_eval_idx),\n",
    "    (_, krp_20jhd_eval_idx),\n",
    "    (_, rrb_19jhd_eval_idx),\n",
    "    (_, rrb_20jhd_eval_idx),\n",
    "    (_, gszh_eval_idx),\n",
    ") in enumerate(\n",
    "    list(\n",
    "        zip(\n",
    "            kf.split(np.zeros(krp_19jhd.num_rows)),\n",
    "            kf.split(np.zeros(krp_20jhd.num_rows)),\n",
    "            kf.split(np.zeros(rrb_19jhd.num_rows)),\n",
    "            kf.split(np.zeros(rrb_20jhd.num_rows)),\n",
    "            kf.split(np.zeros(gszh.num_rows)),\n",
    "        )\n",
    "    )\n",
    "):\n",
    "    print(f\"Fold {i}:\")\n",
    "\n",
    "    # Selecting the test subfolds\n",
    "    krp_19jhd_eval = krp_19jhd.select(krp_19jhd_eval_idx)\n",
    "    krp_20jhd_eval = krp_20jhd.select(krp_20jhd_eval_idx)\n",
    "    rrb_19jhd_eval = rrb_19jhd.select(rrb_19jhd_eval_idx)\n",
    "    rrb_20jhd_eval = rrb_20jhd.select(rrb_20jhd_eval_idx)\n",
    "    gszh_eval = gszh.select(gszh_eval_idx)\n",
    "\n",
    "    # Concatenating the test subfolds\n",
    "    eval_fold = concatenate_datasets(\n",
    "        [krp_19jhd_eval, krp_20jhd_eval, rrb_19jhd_eval, rrb_20jhd_eval, gszh_eval]\n",
    "    )\n",
    "\n",
    "    fold_dataset = DatasetDict(\n",
    "        {\n",
    "            \"eval_all\": eval_fold,\n",
    "            \"eval_krp_19jhd\": krp_19jhd_eval,\n",
    "            \"eval_krp_20jhd\": krp_20jhd_eval,\n",
    "            \"eval_rrb_19jhd\": rrb_19jhd_eval,\n",
    "            \"eval_rrb_20jhd\": rrb_20jhd_eval,\n",
    "            \"eval_gszh\": gszh_eval,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Load baseline SpanMarker model\n",
    "    model = SpanMarkerModel.from_pretrained(\n",
    "        \"stefan-it/span-marker-gelectra-large-germeval14\"\n",
    "    )\n",
    "\n",
    "    # Dummy definitions for optimizer, l_r_scheduler, trainer (we don't train the model here and only use the evaluate method)\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-05)\n",
    "\n",
    "    l_r_scheduler = get_scheduler(\n",
    "        \"polynomial\",\n",
    "        optimizer,\n",
    "        num_warmup_steps=1,\n",
    "        num_training_steps=1,\n",
    "        scheduler_specific_kwargs=dict(lr_end=5e-07, power=3),\n",
    "    )\n",
    "\n",
    "    trainer = NoTrainPreprocTrainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(output_dir=\".\"),\n",
    "        train_dataset=None,\n",
    "        eval_dataset=None,\n",
    "        optimizers=(optimizer, l_r_scheduler),\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Evaluation\n",
    "    #\n",
    "\n",
    "    for pred_ds in [\n",
    "        \"eval_all\",\n",
    "        \"eval_krp_19jhd\",\n",
    "        \"eval_krp_20jhd\",\n",
    "        \"eval_rrb_19jhd\",\n",
    "        \"eval_rrb_20jhd\",\n",
    "        \"eval_gszh\",\n",
    "    ]:\n",
    "        metrics = trainer.evaluate(fold_dataset[pred_ds])\n",
    "\n",
    "        # Filter metrics\n",
    "        metrics_per_ent = {}\n",
    "        for m, v in metrics.items():\n",
    "            if any(\n",
    "                m.endswith(ent_type)\n",
    "                for ent_type in [\n",
    "                    \"PER\",\n",
    "                    \"LOC\",\n",
    "                    \"ORG\",\n",
    "                    \"PERderiv\",\n",
    "                    \"LOCderiv\",\n",
    "                    \"ORGderiv\",\n",
    "                ]\n",
    "            ):\n",
    "                metrics_per_ent[m] = v\n",
    "\n",
    "        # Build metrics dataframe from dict\n",
    "        metrics_per_ent_df = pd.DataFrame.from_dict(metrics_per_ent).reindex(\n",
    "            [\"f1\", \"precision\", \"recall\", \"number\"]\n",
    "        )\n",
    "        metrics_per_ent_df = metrics_per_ent_df.rename(\n",
    "            columns=lambda x: x.split(\"_\")[-1]\n",
    "        )\n",
    "\n",
    "        # Add missing metrics\n",
    "        for col in [\"PER\", \"LOC\", \"ORG\", \"PERderiv\", \"LOCderiv\", \"ORGderiv\"]:\n",
    "            if col not in metrics_per_ent_df.columns:\n",
    "                metrics_per_ent_df[col] = 0.0\n",
    "\n",
    "        # Reorder columns\n",
    "        metrics_per_ent_df = metrics_per_ent_df[\n",
    "            [\"PER\", \"LOC\", \"ORG\", \"PERderiv\", \"LOCderiv\", \"ORGderiv\"]\n",
    "        ]\n",
    "\n",
    "        # Store dataframe for aggregation\n",
    "        metrics_per_ent_dict[pred_ds].append(metrics_per_ent_df)\n",
    "\n",
    "        print(f\"[[ Evaluation {pred_ds} ]]\")\n",
    "        display(metrics_per_ent_df.round(2))\n",
    "\n",
    "    del trainer\n",
    "    del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c159bf9a-8bda-4414-94dc-71ee6eed7682",
   "metadata": {},
   "source": [
    "Generating aggregated metrics across folds (mean and median)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a57167",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dfs = []\n",
    "\n",
    "for pred_ds in [\n",
    "    \"eval_all\",\n",
    "    \"eval_krp_19jhd\",\n",
    "    \"eval_krp_20jhd\",\n",
    "    \"eval_rrb_19jhd\",\n",
    "    \"eval_rrb_20jhd\",\n",
    "    \"eval_gszh\",\n",
    "]:\n",
    "    print(\n",
    "        f\"\"\"-------------------\n",
    "Aggregated metrics ({pred_ds})\n",
    "-------------------\"\"\"\n",
    "    )\n",
    "    metrics_per_ent_conc = pd.concat(metrics_per_ent_dict[pred_ds])\n",
    "    metrics_per_ent_conc = metrics_per_ent_conc.groupby(metrics_per_ent_conc.index)\n",
    "    print(\"Per-ent metrics (mean)\", end=\"\")\n",
    "    mean_df = (\n",
    "        metrics_per_ent_conc.mean()\n",
    "        .round(2)\n",
    "        .reindex([\"f1\", \"precision\", \"recall\", \"number\"])\n",
    "    )\n",
    "    display(mean_df)\n",
    "    mean_dfs.append(\n",
    "        (\n",
    "            pred_ds.replace(\"eval_\", \"\"),\n",
    "            mean_df.rename({\"f1\": \"F1\", \"precision\": \"P\", \"recall\": \"R\"}),\n",
    "        )\n",
    "    )\n",
    "    print(\"Per-ent metrics (median)\", end=\"\")\n",
    "    median_df = (\n",
    "        metrics_per_ent_conc.median()\n",
    "        .round(2)\n",
    "        .reindex([\"f1\", \"precision\", \"recall\", \"number\"])\n",
    "    )\n",
    "    display(median_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d79adf-c3fa-4775-88a3-d0798c180c95",
   "metadata": {},
   "source": [
    "Some dense pandas code to get a heatmap table of all aggregated metrics (mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1f944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [t[1].drop(\"number\") for t in mean_dfs], keys=[t[0] for t in mean_dfs]\n",
    ").T.style.format(\"{:.2f}\").background_gradient(\n",
    "    cmap=\"RdYlGn\", vmin=0.0, vmax=1.0\n",
    ").set_properties(**{\"font-size\": \"12px\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f7241-40cb-4f22-9a91-71dc434ec2dc",
   "metadata": {},
   "source": [
    "The same for the mean frequency of named entity types per fold and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fdec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_freq_df = pd.concat(\n",
    "    [t[1].drop([\"F1\", \"P\", \"R\"]) for t in mean_dfs], keys=[t[0] for t in mean_dfs]\n",
    ").T.droplevel(1, axis=\"columns\")\n",
    "entity_freq_df.style.format(\"{:.2f}\").background_gradient(\n",
    "    cmap=\"RdYlGn\", vmin=entity_freq_df.min().min(), vmax=entity_freq_df.max().max()\n",
    ").set_properties(**{\"font-size\": \"12px\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
